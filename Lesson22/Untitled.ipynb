{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1720810-57d1-43ea-b707-8e3f195fa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff711716-39c1-41b8-8cf5-f97f6f560966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2f7f7e-24f6-48e8-9fe7-7663c0424223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/sv/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f4fecc-d056-4b4e-8cc8-53f83f220d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4c7a05-3120-4589-a779-4fdca4cb6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one(text):\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934ed049-47d8-4289-ac1d-c9e142bc0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08133224587104161"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c75b0a-3b80-4594-9529-2b6222fdeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def answer_two(text): \n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    whale_count = sum([1 for token in tokens if token.lower() == 'whale'])\n",
    "    \n",
    "    total_tokens = len(tokens)\n",
    "    return (whale_count / total_tokens) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074a0ff5-772a-4582-a5b5-9eff6d8065b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42583559452295433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c350d78c-b500-4296-a3dc-0bde11a3009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def answer_three(text):\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    return freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7932efef-8f2d-45e3-bd48-803932387562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_three(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471cb658-5225-4af7-a932-274e0fa94be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def answer_four(text, min_length=6, min_frequency=150):\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bag_of_words = Counter(tokens)\n",
    "    \n",
    "    return sorted([x[0] for x in bag_of_words.most_common() if len(x[0]) > 5 and x[1] > 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d386a3f-a703-4a22-b187-f804e76e4994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_four(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f1f2eed-cf8e-4ff3-92cf-3081e820a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    longest_word = max(tokens, key=len)\n",
    "    return longest_word, len(longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7ef66d-7e13-46d6-894a-ae3f35447a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf5239f-905f-4fba-a9be-921b6a28e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six(text, min_frequency=2000):\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    word_frequencies = Counter(tokens)\n",
    "    return [(word, freq) for word, freq in word_frequencies.items() if word.isalpha() and freq > min_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "797bad91-c254-4c83-be44-d948efa0381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4698),\n",
       " ('to', 4597),\n",
       " ('the', 14422),\n",
       " ('in', 4163),\n",
       " ('and', 6414),\n",
       " ('i', 2101),\n",
       " ('his', 2530),\n",
       " ('of', 6586),\n",
       " ('it', 2508),\n",
       " ('that', 3081)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_six(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85a9d6a-fb77-4e52-aa0d-163791248420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def answer_seven(text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_tokens = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    return total_tokens / len(sentences) if len(sentences) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99477124-00e5-4dab-a4b3-9dd5643a3e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.88591149005278"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_seven(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe8406d-8cc6-40f2-b463-479fbd824de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sv/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def answer_eight(text, n=5):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_freq = Counter(tag for word, tag in pos_tags)\n",
    "    return pos_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03799bb7-d76a-47b3-88bc-c4e00c0fb70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 32727), ('IN', 28662), ('DT', 25879), (',', 19204), ('JJ', 17613)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eight(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63e86f4f-40ad-4ee8-ba27-11c8a2b30c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/sv/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cormulent_recommendation: corpulent', 'incendenece_recommendation: intendence', 'validrate_recommendation: validate']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "correct_spellings = words.words()\n",
    "\n",
    "def spelling_recommender(misspelled_words):\n",
    "    recommendations = []\n",
    "\n",
    "    for misspelled_word in misspelled_words:\n",
    "        candidates = [word for word in correct_spellings if word.startswith(misspelled_word[0])]\n",
    "        recommended_word = min(candidates, key=lambda word: edit_distance(misspelled_word, word, transpositions=True))\n",
    "        recommendations.append(f'{misspelled_word}_recommendation: {recommended_word}')\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "misspelled_words = ['cormulent', 'incendenece', 'validrate']\n",
    "recommendations = spelling_recommender(misspelled_words)\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
