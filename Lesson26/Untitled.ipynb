{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d314a-caa5-4a61-89b7-d8d833dd70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'DATA')\n",
    "def load_dataset():\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "\n",
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()\n",
    "index = 11\n",
    "print(classes[Y_train[index]])\n",
    "plt.imshow(X_train[index])\n",
    "plt.show()\n",
    "print('X_train.shape= ', X_train.shape)\n",
    "print('X_test.shape= ', X_test.shape)\n",
    "print('Y_train.shape= ', Y_train.shape)\n",
    "print('Y_test.shape= ', Y_test.shape)\n",
    "print('Number of training examples: ', X_train.shape[0])\n",
    "print('Number of testing examples: ', X_test.shape[0])\n",
    "print('Height/Width of each image: ', X_train.shape[1])\n",
    "print('Each image is of size: ', X_train.shape[1:])\n",
    "# Reshape the data \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test = Y_test.reshape(1, -1)\n",
    "print('X_train_flatten shape: ', X_train_flatten.shape)\n",
    "print('Y_train shape: ', Y_train.shape)\n",
    "print('X_test_flatten shape: ', X_test_flatten.shape)\n",
    "print('Y_test shape: ', Y_test.shape)\n",
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255.\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims - list containing the dimensions of each layer in our network including input layer e.g. [12288,7,1]\n",
    "    Returns: dictionary with keys \"W\" and \"b\" and their values are dicts with keys corresponding to layers numbers.\n",
    "        for 'W' - value for every layer is weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        for 'b' - bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"    \n",
    "    np.random.seed(1)\n",
    "    parameters = {'W':{}, 'b':{}}\n",
    "\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W'][l] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b'][l] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n",
    "# check the initialize_parameters()\n",
    "layer_dims= [2,3,5,1] \n",
    "params = initialize_parameters(layer_dims)\n",
    "for l in range(1,len(layer_dims)):\n",
    "    print ('W[{0}] =\\n{1}\\nb[{0}] =\\n{2}\\n'.format(l, params['W'][l], params['b'][l] ))\n",
    "def forward_propagation_step(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    A_prev - activations from previous layer: (size of previous layer, number of examples)\n",
    "    W - weights matrix: array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector, array of shape (size of the current layer, 1)\n",
    "    activation - text string \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -  post-activation value \n",
    "    cache - tuple containing W, b, A_prev, Z stored for computing the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A= sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    cache = (W, b, A_prev, Z) # used at backward propagation. Note: b looks as need just to check the shape of dJ_db\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    A_last - vector of predicted probabilties - activations of last layer L, shape (1, number of examples)\n",
    "    Y - true label e.g. cat vs non-cat, shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    assert (A_last.shape == Y.shape)\n",
    "    cost = -1 / Y.shape[1] * np.sum(Y * np.log(A_last) + (1 - Y) * np.log(1 - A_last))\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    assert(cost.shape == ()) \n",
    "    \n",
    "    return cost\n",
    "def init_backward_propagation(Y, A_last):\n",
    "    dL_dA_last =  - (np.divide(Y, A_last) - np.divide(1 - Y, 1 - A_last)) / Y.shape[1]\n",
    "    return dL_dA_last\n",
    "def backward_propagation_step(dL_dA, cache, activation):\n",
    "    \"\"\"\n",
    "    dL_dA - activation gradient for current layer l\n",
    "    cache - (W, b, A_prev, Z) stored for current layer  l\n",
    "    activation - string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dA_prev - Gradient activation of the previous layer l-1, same shape as A_prev\n",
    "    dL_dW - Gradient of W current layer l, same shape as W\n",
    "    dL_db - Gradient of b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    W, b, A_prev, Z = cache \n",
    "\n",
    "    # backward activation part:\n",
    "    if activation == \"relu\":\n",
    "        dg_dz = relu_backward(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dg_dz = sigmoid_backward(Z)\n",
    "        \n",
    "    assert (dL_dA.shape == dg_dz.shape)\n",
    "    dL_dZ = dL_dA * dg_dz\n",
    "\n",
    "    # backward linear part:\n",
    "   \n",
    "    dL_dW = 1 / A_prev.shape[1] * np.dot(dL_dZ, A_prev.T)\n",
    "    dL_db = 1 / A_prev.shape[1] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dA_prev = np.dot(W.T, dL_dZ)\n",
    "    \n",
    "\n",
    "    assert (dL_dA_prev.shape == A_prev.shape)\n",
    "    assert (dL_dW.shape == W.shape)\n",
    "    assert (dL_db.shape == b.shape)\n",
    "\n",
    "    return dL_dA_prev, dL_dW, dL_db\n",
    "        \n",
    "\n",
    "def relu_backward(Z):\n",
    "    dg_dz = np.where(Z > 0, 1, 0)\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(Z):\n",
    "\n",
    "    dg_dz = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters due to gradient descent rule \n",
    "    parameters - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "    grads - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "   \n",
    "    Returns: updated parameters the same shape as input parameters \n",
    "    \"\"\"\n",
    "   \n",
    "    for i in range(1, len(parameters['W']) + 1):\n",
    "        parameters['W'][i] -= learning_rate * grads['W'][i]\n",
    "        parameters['b'][i] -= learning_rate * grads['b'][i]\n",
    "    return parameters\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    '''\n",
    "    X - input layer of shape (input size, number of examples)\n",
    "    Y - output layer of shape (1,m)\n",
    "    layers_dims - list of layers dims including input layer \n",
    "    '''\n",
    "  \n",
    "    np.random.seed(1)\n",
    "    grads = {'W':{}, 'b':{}}\n",
    "    costs = []   # track the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "\n",
    "    # Initialize parameters \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        A1, cache1 = forward_propagation_step(X, parameters['W'][1], parameters['b'][1], activation='relu')\n",
    "        A2, cache2 = forward_propagation_step(A1, parameters['W'][2], parameters['b'][2], activation='sigmoid')\n",
    "\n",
    "        # Compute cost        \n",
    "        cost = compute_cost(A2, Y)\n",
    "                \n",
    "        # Initialize backward propagation        \n",
    "        dL_dA2 = init_backward_propagation(Y, A2)\n",
    "\n",
    "        # Backward propagation.\n",
    "        dL_dA1, grads['W'][2], grads['b'][2] = backward_propagation_step(dL_dA2, cache2, activation='sigmoid')\n",
    "        _, grads['W'][1], grads['b'][1] = backward_propagation_step(dL_dA1, cache1, activation='relu')\n",
    "        \n",
    "       \n",
    "        # Update parameters    \n",
    "        parameters =  update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    if print_cost:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('Втрати')\n",
    "        plt.xlabel('Ітерації (сотні)')\n",
    "        plt.title(\"Швидкість навчання =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return parameters\n",
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = two_layer_model(\n",
    "    X_train_scaled, Y_train, layers_dims, learning_rate = 0.003, num_iterations = 3000, print_cost=True)\n",
    "def evaluate_two_layers(X, Y, parameters):\n",
    "    \"\"\"        \n",
    "    X - array to predict \n",
    "    parameters - parameters of the trained model\n",
    "    \"\"\"\n",
    "  \n",
    "    # Forward propagation\n",
    "    A1,_ = forward_propagation_step(X, parameters['W'][1], parameters['b'][1], activation='relu')\n",
    "    Y_pred, _ = forward_propagation_step(A1, parameters['W'][2], parameters['b'][2], activation='sigmoid')\n",
    "\n",
    "    predictions = (Y_pred > 0.5)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    return accuracy   \n",
    "\n",
    "print(\"Accuracy:: {:.3f}\".format(evaluate_two_layers(X_train_scaled, Y_train, parameters)))\n",
    "print(\"Accuracy:: {:.3f}\".format(evaluate_two_layers(X_test_scaled, Y_test, parameters)))\n",
    "\n",
    "def forward_propagation_whole_process(X, parameters):\n",
    "    \"\"\"\n",
    "    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    X - data, array of shape (input size, number of examples)\n",
    "    parameters - initialized parameters foreach of 'W' and 'b' keas values have keys 1,2,...L \n",
    "    \n",
    "    Returns:\n",
    "    A_last - last activation value (y_pred)\n",
    "    caches - dict of caches containing every cache of forward propagation indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters['W']) # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters['W'][l], A) + parameters['b'][l]\n",
    "        A = np.maximum(0, Z)\n",
    "        cache = (A, Z)\n",
    "        caches[l] = cache\n",
    "\n",
    "    \n",
    "    #LINEAR -> SIGMOID\n",
    "    Z = np.dot(parameters['W'][L], A) + parameters['b'][L]  # LINEAR\n",
    "    A_last = sigmoid(Z)\n",
    "    caches[L] = (A, Z)\n",
    "\n",
    "    assert(A_last.shape == (1, X.shape[1])) # (1,m) \n",
    "            \n",
    "    return A_last, caches\n",
    "def backward_propagation_whole_process(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    A_last - probability vector, output(y_pred) of the forward propagation \n",
    "    Y - true labels (0 if non-cat, 1 if cat)\n",
    "    caches - dict of caches for each layer that contains (W, b, A, Z)\n",
    "    Returns: grads - of keys 'W' and 'b' each containing the  dictionaries of keys 1..L  \n",
    "    \"\"\"\n",
    "    dL_dA= {}\n",
    "    dL_dW = {}\n",
    "    dL_db= {}\n",
    "    \n",
    "    L = len(caches) # the number of layers\n",
    "    m = Y.shape[1] # number of samples\n",
    "    Y = Y.reshape(A_last.shape) # make sure Y is the same shape as A_last(y_pred)\n",
    "    \n",
    "    # Initialize backpropagation\n",
    "    dL_dA[L] = - (np.divide(A_last, Y) - np.divide(1 - A_last, 1 - Y))\n",
    "\n",
    "    # Backward pass for layer (SIGMOID -> LINEAR)\n",
    "    current_cache = caches[L]\n",
    "    A_prev, Z_prev = current_cache\n",
    "\n",
    "    dL_dW[L] = np.dot(dL_dA[L], A_prev.T) / m\n",
    "    dL_db[L] = np.sum(dL_dA[L], axis=1, keepdims=True) / m\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(1, L)):\n",
    "      # Backward pass for l-th layer (RELU -> LINEAR)\n",
    "      current_cache = caches[l]\n",
    "      A_prev, Z_prev = current_cache\n",
    "\n",
    "      # ReLU gradient (derivative)\n",
    "      g = np.copy(Z_prev)\n",
    "      g[Z_prev < 0] = 0\n",
    "\n",
    "      dL_dA[l-1] = np.dot(dL_dW[l], A_prev.T) / m + np.dot(g.T, dL_dA[l]) * dA_dZ(Z_prev)  # Chain rule\n",
    "      dL_dW[l] = np.dot(dL_dA[l-1], A_prev.T) / m\n",
    "      dL_db[l] = np.sum(dL_dA[l-1], axis=1, keepdims=True) / m\n",
    "\n",
    "    grads = {\"W\": dL_dW, \"b\": dL_db}\n",
    "\n",
    "    return grads\n",
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, verbose = False):\n",
    "    \"\"\"\n",
    "    X - data, array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y - true label vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims - list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate - learning rate of the gradient descent update rule\n",
    "    num_iterations - number of iterations of the optimization loop\n",
    "    verbose - if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters - parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    print ('Training {}-layers neural network with layers dimensions: {}'.format (len(layers_dims)-1, layers_dims))\n",
    "    np.random.seed(1)\n",
    "    costs = [] # to track of cost\n",
    "            \n",
    "    parameters = None\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        A_last, caches = None\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = None\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = None\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    None\n",
    "    \n",
    "   \n",
    "    return parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
