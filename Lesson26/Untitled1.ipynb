{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e1783-452d-431a-96d2-447fa55a4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_step(dL_dA, cache, activation):\n",
    "    \"\"\"\n",
    "    dL_dA - activation gradient for current layer l\n",
    "    cache - (W, b, A_prev, Z) stored for current layer  l\n",
    "    activation - string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dA_prev - Gradient activation of the previous layer l-1, same shape as A_prev\n",
    "    dL_dW - Gradient of W current layer l, same shape as W\n",
    "    dL_db - Gradient of b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    W, b, A_prev, Z = cache \n",
    "\n",
    "    # backward activation part:\n",
    "    if activation == \"relu\":\n",
    "        dg_dz = relu_backward(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dg_dz = sigmoid_backward(Z)\n",
    "        \n",
    "    assert (dL_dA.shape == dg_dz.shape)\n",
    "    dL_dZ = dL_dA * dg_dz\n",
    "\n",
    "    # backward linear part:\n",
    "   \n",
    "    dL_dW = 1 / A_prev.shape[1] * np.dot(dL_dZ, A_prev.T)\n",
    "    dL_db = 1 / A_prev.shape[1] * np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dA_prev = np.dot(W.T, dL_dZ)\n",
    "    \n",
    "\n",
    "    assert (dL_dA_prev.shape == A_prev.shape)\n",
    "    assert (dL_dW.shape == W.shape)\n",
    "    assert (dL_db.shape == b.shape)\n",
    "\n",
    "    return dL_dA_prev, dL_dW, dL_db\n",
    "        \n",
    "\n",
    "def relu_backward(Z):\n",
    "    dg_dz = np.where(Z > 0, 1, 0)\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(Z):\n",
    "\n",
    "    dg_dz = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "\n",
    "\n",
    "\n",
    "def forward_propagation_whole_process(X, parameters):\n",
    "    \"\"\"\n",
    "    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    X - data, array of shape (input size, number of examples)\n",
    "    parameters - initialized parameters foreach of 'W' and 'b' keas values have keys 1,2,...L \n",
    "    \n",
    "    Returns:\n",
    "    A_last - last activation value (y_pred)\n",
    "    caches - dict of caches containing every cache of forward propagation indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters['W']) # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters['W'][l], A) + parameters['b'][l]\n",
    "        A = np.maximum(0, Z)\n",
    "        cache = (A, Z)\n",
    "        caches[l] = cache\n",
    "\n",
    "    \n",
    "    #LINEAR -> SIGMOID\n",
    "    Z = np.dot(parameters['W'][L], A) + parameters['b'][L]  # LINEAR\n",
    "    A_last = sigmoid(Z)\n",
    "    caches[L] = (A, Z)\n",
    "\n",
    "    assert(A_last.shape == (1, X.shape[1])) # (1,m) \n",
    "            \n",
    "    return A_last, caches\n",
    "\n",
    "\n",
    "def backward_propagation_whole_process(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    A_last - probability vector, output(y_pred) of the forward propagation \n",
    "    Y - true labels (0 if non-cat, 1 if cat)\n",
    "    caches - dict of caches for each layer that contains (W, b, A, Z)\n",
    "    Returns: grads - of keys 'W' and 'b' each containing the  dictionaries of keys 1..L  \n",
    "    \"\"\"\n",
    "    dL_dA= {}\n",
    "    dL_dW = {}\n",
    "    dL_db= {}\n",
    "    \n",
    "    L = len(caches) # the number of layers\n",
    "    m = A_last.shape[1] # number of samples\n",
    "    #Y = Y.reshape(A_last.shape) # make sure Y is the same shape as A_last(y_pred)\n",
    "    \n",
    "    # Initialize the backpropagation    \n",
    "    dL_dA[L] = - (np.divide(Y, A_last) - np.divide(1 - Y, 1 - A_last))\n",
    "\n",
    "    # layer (SIGMOID -> LINEAR) gradients\n",
    "    current_cache = caches[L-1]\n",
    "    dL_dA[L-1], dL_dW[L], dL_db[L] = backward_propagation_step(dL_dA[L], current_cache, activation='sigmoid')\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(1,L)): #  starts with L-1 ends with 1 \n",
    "        # l-th layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l-1]\n",
    "        dL_dA[l-1], dL_dW[l], dL_db[l] = backward_propagation_step(dL_dA[l], current_cache, activation='relu')\n",
    "        \n",
    "    grads = {'dW': dL_dW, 'db': dL_db}\n",
    "    \n",
    "    return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
